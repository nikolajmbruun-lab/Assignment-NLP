torch==2.9.1+cu126
torchvision==0.24.1+cu126
torchaudio==2.9.1+cu126
transformers==4.57.1
accelerate==1.11.0
bitsandbytes==0.48.2

datasets==4.4.1
pandas==2.3.2
numpy==2.3.2
networkx==3.5
matplotlib==3.10.6
seaborn==0.13.2
tqdm==4.67.1

pydantic==2.12.4
scikit-learn==1.7.2

openai==2.7.2
google-generativeai==0.8.5
groq==0.34.0

python-louvain==0.16

Right now the LLM extraction runs locally and needs a GPU that can handle a model like Llama 3.1 8B Instruct in 4 bit or 8 bit. The PyTorch version I use (torch==2.9.1+cu126) is built for CUDA 12.6 so your GPU and drivers need to match that for it to work.

If you do not have a GPU the extraction part will basically fail. In that case you can swap it out and just call an LLM API instead for example:

OpenAI

Groq

Google Gemini

The main thing is that the model you use returns the same kind of structure topics entities sentiment and summary. You can also use a different local instruct model if you want as long as you change the loading code and keep the output format the same.